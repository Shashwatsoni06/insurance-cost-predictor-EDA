# -*- coding: utf-8 -*-
"""Insurence Machine Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J9ESSGDiLv9Mf8AZxS8MLg0oapiLFDvS
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/insurance.csv')
df

"""# EDA"""

df.shape

df.head()

df.info()

df.describe()

df.isnull().sum()

df.columns

numeric_columns = ['age', 'bmi', 'children', 'charges']
for col in numeric_columns:
    plt.figure(figsize = (6,4))
    sns.histplot(df[col], kde = True, bins= 20)

sns.countplot( x= df ['children'])

sns.countplot(x = df['sex'])

sns.countplot(x= df['smoker'])

for col in numeric_columns:
    plt.figure(figsize = (6,4))
    sns.boxplot(x = df[col])

plt.figure(figsize=(8,6))
sns.heatmap(df.corr(numeric_only=True),annot=True)

"""# Data Cleaning and Preprocessing"""

df_cleaned = df.copy()

df_cleaned.head()

df_cleaned.drop_duplicates(inplace = True)

df_cleaned.shape

df_cleaned.isnull().sum()

df_cleaned.dtypes

df_cleaned['sex'].value_counts() # to check, male, MALE, mALE Different way of writing  but it okay in this data set_env

# label encoding converting data into numerical value, i.e, 0, 1
df_cleaned['sex'] = df_cleaned['sex'].map({"male" : 0, "female": 1})

df_cleaned.head()

df_cleaned['smoker'].value_counts()

df_cleaned['smoker'] = df_cleaned['smoker'].map({"no" : 0, "yes": 1})

df_cleaned.head()

df_cleaned.rename(columns={
    'sex': 'is_female',
    'smoker': "is_smoker",
},inplace=True)
# to rename the sex and smoker columns

df_cleaned.head()

df_cleaned['region'].value_counts()

# we will have to now convert the region data into an integer type, i.e, 0, 1
# We will do this with hot line coding. In hot line coding, we make separate columns and give 0 or 1 as a value, here we will make 4 columns beacuse their are 4 different type of value count
df_cleaned = pd.get_dummies(df_cleaned, columns = ['region'],drop_first=True)

df_cleaned.head()

df_cleaned = df_cleaned.astype(int)
# here we will now convert all true and false values of 4 columns of region into 0 and 1

df_cleaned.head()

"""# Feature Engineering and Extraction"""

sns.histplot(df['bmi'])

df_cleaned['bmi_category'] = pd.cut(
    df_cleaned['bmi'],
    bins=[0, 18.5, 24.9, 29.9, float('inf')],
    labels=['Underweight', 'Normal', 'Overweight', 'Obese']

)

df_cleaned

df_cleaned = pd.get_dummies(df_cleaned, columns = ['bmi_category'],drop_first=True)

df_cleaned = df_cleaned.astype(int)

df_cleaned.head()

df_cleaned.columns

# feature scaling

from sklearn.preprocessing import StandardScaler
cols = ['age', 'bmi', 'children']
scaler = StandardScaler()

df_cleaned[cols] = scaler.fit_transform(df_cleaned[cols])

df_cleaned.head()

# list of features to check against target [corelation with target]

from scipy.stats import pearsonr
# ---------------------------------
# pearson correlation calculationabs
# -----------------------------------

selected_features = [
    'age', 'is_female', 'bmi', 'children', 'is_smoker', 'charges',
       'region_northwest', 'region_southeast', 'region_southwest',
       'bmi_category_Normal', 'bmi_category_Overweight', 'bmi_category_Obese'
    ]
correlations = {
    feature: pearsonr(df_cleaned[feature], df_cleaned['charges'])[0]
    for feature in selected_features
}

correlation_df = pd.DataFrame(list(correlations.items()), columns=['feature','Pearson Correlation'])
correlation_df.sort_values(by='Pearson Correlation', ascending=False)

cat_features = [
    'is_female', 'is_smoker',
    'region_northwest', 'region_southwest', 'region_southwest',
    'bmi_category_Normal', 'bmi_category_Overweight', 'bmi_category_Obese'
]

from scipy.stats import chi2_contingency
import pandas as pd

alpha = 0.05

# Create binned target variable
df_cleaned['charges_bin'] = pd.qcut(df_cleaned['charges'], q=4, labels=False)

chi2_results = {}

for col in cat_features:
    contingency = pd.crosstab(df_cleaned[col], df_cleaned['charges_bin'])
    chi2_stat, p_val, _, _ = chi2_contingency(contingency)
    decision = 'Reject Null (Keep Feature)' if p_val < alpha else 'Accept Null (Drop Feature)'
    chi2_results[col] = {
        'chi2_statistic': chi2_stat,
        'p_value': p_val,
        'Decision': decision
    }

# Convert dict to DataFrame
chi2_df = pd.DataFrame(chi2_results).T

# Sort by p-value
chi2_df = chi2_df.sort_values(by='p_value')

# Display result
chi2_df

final_df = df_cleaned[['age', 'is_female', 'bmi', 'children', 'is_smoker', 'charges','region_southeast', 'region_southwest', 'bmi_category_Obese']]

final_df

from sklearn.model_selection import train_test_split

X = final_df.drop('charges', axis = 1)
y = final_df['charges']
# in this code we drop charges and put in 'x'
# and we put charges data in 'y' because charges data is output data(to perdict)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

y_pred

from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred)
print("R-squared:", r2)
n = X_test.shape[0]
p = X_test.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print("Adjusted R-squared:", adjusted_r2)

